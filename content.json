{"meta":{"title":"����ɭ","subtitle":"�ƣ�","description":"������Ϣ���ܶ���ֹ","author":"yun","url":"http://example.com","root":"/"},"pages":[{"title":"Repositories","date":"2022-01-16T07:45:02.914Z","updated":"2022-01-16T07:05:11.540Z","comments":false,"path":"repository/index.html","permalink":"http://example.com/repository/index.html","excerpt":"","text":""},{"title":"分类","date":"2022-01-13T15:43:54.000Z","updated":"2022-01-17T15:40:51.107Z","comments":false,"path":"categories/index.html","permalink":"http://example.com/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-01-13T15:43:17.000Z","updated":"2022-01-17T15:54:22.492Z","comments":false,"path":"tags/index.html","permalink":"http://example.com/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Keepalived-双机被热","slug":"Keepalived-双机被热","date":"2022-01-21T12:38:38.000Z","updated":"2022-01-21T13:09:43.825Z","comments":true,"path":"2022/01/21/Keepalived-双机被热/","link":"","permalink":"http://example.com/2022/01/21/Keepalived-%E5%8F%8C%E6%9C%BA%E8%A2%AB%E7%83%AD/","excerpt":"Keepalived高可用故障切换转移1、keepalived基础知识Keepalived软件起初是专为LVS负载均衡软件设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了可以实现高可用的VRRP功能。因此，Keepalived除了能够管理LVS软件外，还可以作为其他服务（例如：Nginx、Haproxy、MySQL等）的高可用解决方案软件。","text":"Keepalived高可用故障切换转移1、keepalived基础知识Keepalived软件起初是专为LVS负载均衡软件设计的，用来管理并监控LVS集群系统中各个服务节点的状态，后来又加入了可以实现高可用的VRRP功能。因此，Keepalived除了能够管理LVS软件外，还可以作为其他服务（例如：Nginx、Haproxy、MySQL等）的高可用解决方案软件。 https://www.cnblogs.com/clsn/p/8052649.html Keepalived软件主要是通过VRRP协议实现高可用功能的。VRRP是Virtual Router RedundancyProtocol(虚拟路由器冗余协议）的缩写，VRRP出现的目的就是为了解决静态路由单点故障问题的，它能够保证当个别节点宕机时，整个网络可以不间断地运行。 所以，Keepalived 一方面具有配置管理LVS的功能，同时还具有对LVS下面节点进行健康检查的功能，另一方面也可实现系统网络服务的高可用功能。 2、VRRP工作过程https://www.cnblogs.com/coprince/p/6699716.html keepalived服务的三个重要功能​ 管理LVS负载均衡软件 实现LVS集群节点的健康检查中 作为系统网络服务的高可用性（failover） 3.1.1 双机热备keepalived1、实验环境 IP地址 主机名 节点角色 服务器 192.168.200.11 node1 keeplived master http1 192.168.200.12 node2 keeplived backup http2 2、启用httpd服务（主备节点）1234567[root@node1 2~]# yum install -y httpd[root@node1 2~]# systemctl enable httpd &amp;&amp; systemctl start httpd[root@node1 ~]# echo &quot;Hello 192.168.200.11&quot; &gt; /var/www/html/index.html[root@node1 ~]# curl 192.168.200.11[root@node2 ~]# echo &quot;Hello 192.168.200.12&quot; &gt; /var/www/html/index.html[root@node2 ~]# curl 192.168.200.12 2、安装keepalived（主备节点）①yum方式安装12[root@node1 ~]# yum install -y keepalived[root@node2 ~]# yum install -y keepalived ②源码安装（参考）http://www.keepalived.org 1 3、配置文件12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758##主节点node1[root@node1 ~]# vim /etc/keepalived/keepalived.conf! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from root@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id MASTER&#125;vrrp_instance VI_1 &#123; state MASTER interface ens33 virtual_router_id 51 priority 100 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.200.60 &#125;&#125;[root@node1 ~]# scp /etc/keepalived/keepalived.conf root@192.168.200.12:/etc/keepalived/##备份节点node2! Configuration File for keepalivedglobal_defs &#123; notification_email &#123; root@localhost &#125; notification_email_from root@localhost smtp_server 127.0.0.1 smtp_connect_timeout 30 router_id BACKUP&#125;vrrp_instance VI_1 &#123; state BACKUP interface ens33 virtual_router_id 51 priority 90 advert_int 1 authentication &#123; auth_type PASS auth_pass 1111 &#125; virtual_ipaddress &#123; 192.168.200.60 &#125;&#125; 4、启动服务，验证状态123456789101112131415161718192021##主节点有虚拟IP，备份节点没有虚拟IP[root@node1 ~]# systemctl enable keepalived &amp;&amp; systemctl start keepalived[root@node1 ~]# ip add list ens332: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:ab:0e:0e brd ff:ff:ff:ff:ff:ff inet 192.168.200.11/24 brd 192.168.200.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.200.60/32 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:feab:e0e/64 scope link valid_lft forever preferred_lft forever[root@node2 ~]# systemctl enable keepalived &amp;&amp; systemctl start keepalived[root@node2 ~]# ip add list ens332: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:28:0c:72 brd ff:ff:ff:ff:ff:ff inet 192.168.200.12/24 brd 192.168.200.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe28:c72/64 scope link valid_lft forever preferred_lft forever 5、漂移测试123456789101112131415161718192021222324[root@node1 ~]# systemctl stop keepalived.service [root@node1 ~]# ip a[root@node2 ~]# ip a1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever2: ens33: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc pfifo_fast state UP group default qlen 1000 link/ether 00:0c:29:28:0c:72 brd ff:ff:ff:ff:ff:ff inet 192.168.200.12/24 brd 192.168.200.255 scope global noprefixroute ens33 valid_lft forever preferred_lft forever inet 192.168.200.60/32 scope global ens33 valid_lft forever preferred_lft forever inet6 fe80::20c:29ff:fe28:c72/64 scope link valid_lft forever preferred_lft forever ##思考，主节点故障恢复后，vip会漂移到哪台主机[root@node2 ~]# curl http://192.168.200.60","categories":[],"tags":[]},{"title":"mariadb-galera-cluster","slug":"mariadb-galera-cluster","date":"2022-01-20T14:09:27.000Z","updated":"2022-01-20T14:13:36.437Z","comments":true,"path":"2022/01/20/mariadb-galera-cluster/","link":"","permalink":"http://example.com/2022/01/20/mariadb-galera-cluster/","excerpt":"2.2 MariaDB Galera ClusterGalera Cluster是集成了Galera插件的MySQL集群，是一种新型的，数据不共享的，高度冗余的高可用方案，目前Galera Cluster有两个版本，分别是Percona Xtradb Cluster和MariaDB Cluster，Galera本身具有多主特性，即采用Multi-master的集群架构，是一个即稳健，又在数据一致性、完整性及高性能方面有出色表现的高可用解决方案","text":"2.2 MariaDB Galera ClusterGalera Cluster是集成了Galera插件的MySQL集群，是一种新型的，数据不共享的，高度冗余的高可用方案，目前Galera Cluster有两个版本，分别是Percona Xtradb Cluster和MariaDB Cluster，Galera本身具有多主特性，即采用Multi-master的集群架构，是一个即稳健，又在数据一致性、完整性及高性能方面有出色表现的高可用解决方案 1、实验环境 IP地址 主机名 节点角色 192.168.100.11 node1 数据库节点 192.168.100.12 node2 数据库节点 192.168.100.13 node3 数据库节点 2、实验主机基础配置（node1、node2、node3台都做）①关闭防火墙和selinxu 1234[root@node1 ~]# systemctl disable --now firewalld[root@node1 ~]# sed -i &#x27;s/^SELINUX=enforcing$/SELINUX=disabled/&#x27; /etc/selinux/config &amp;&amp; setenforce 0##node2、node3相同操作 ②配置主机名解析 123456789101112131415[root@node1 ~]# cat &gt;&gt;/etc/hosts &lt;&lt;EOF192.168.200.11 node1192.168.200.12 node2192.168.200.13 node3EOF[root@node1 ~]# cat /etc/hosts##ssh免密登录[root@node1 ~]# ssh-keygen //一路回车[root@node1 ~]# ssh-copy-id node2 //yes 000000[root@node1 ~]# ssh-copy-id node3##node1往node2、node3加密拷贝[root@node1 ~]# scp /etc/hosts root@node2:/etc[root@node1 ~]# scp /etc/hosts root@node3:/etc ③安装mariadb 参考文档 https://mariadb.com/kb/en/yum/#installing-mariadb-galera-cluster-with-yum 123456789101112131415161718##配置yum仓库[root@node1 ~]# cat &gt; /etc/yum.repos.d/mariadb.repo &lt;&lt;EOF[mariadb]name = MariaDBbaseurl = http://yum.mariadb.org/10.3/centos7-amd64gpgkey=https://yum.mariadb.org/RPM-GPG-KEY-MariaDBgpgcheck=1EOF#替换为中科大源[root@node1 ~]# sed -i &#x27;s#yum\\.mariadb\\.org#mirrors.ustc.edu.cn/mariadb/yum#&#x27; /etc/yum.repos.d/mariadb.repo[root@node1 ~]# scp /etc/yum.repos.d/mariadb.repo node2:/etc/yum.repos.d/[root@node1 ~]# scp /etc/yum.repos.d/mariadb.repo node3:/etc/yum.repos.d/#安装mariadb、galera和rsync,其中galera作为依赖自动安装[root@node1 ~]# yum install -y MariaDB-server MariaDB-client##node2、node3相同操作 3、 启动数据库并安全初始化（node1、node2、node3）1234567891011[root@node1 ~]# systemctl enable mariadb &amp;&amp; systemctl start mariadb[root@node1 ~]# mysql_secure_installation回车-y-000000-000000-y-n-y-y[root@node2 ~]# systemctl enable mariadb &amp;&amp; systemctl start mariadb[root@node2 ~]# mysql_secure_installation回车-y-000000-000000-y-n-y-y[root@node3 ~]# systemctl enable mariadb &amp;&amp; systemctl start mariadb[root@node3 ~]# mysql_secure_installation回车-y-000000-000000-y-n-y-y 4、 修改数据库配置文件（node1、node2、node3） 参考文档 1、wsrep配置选项 https://www.cnblogs.com/zhongguiyao/p/14146416.html 123456789101112131415161718192021mv /etc/my.cnf.d/server.cnf /media/ #移动配置文件vi /etc/my.cnf.d/server.cnf[galera]wsrep_on=ON #启用复制wsrep_provider=/usr/lib64/galera/libgalera_smm.so #wsrep库的位置wsrep_cluster_address=&quot;gcomm://192.168.200.11,192.168.200.12,192.168.200.13&quot; #启动时要连接的群集节点的地址wsrep_node_name= node1 #节点名，其他节点改为node2、node3wsrep_node_address=192.168.200.11 #节点IP，node2、node3改为自己IPbinlog_format=rowdefault_storage_engine=InnoDBinnodb_autoinc_lock_mode=2wsrep_slave_threads=1innodb_flush_log_at_trx_commit=0innodb_buffer_pool_size=120Mwsrep_sst_method=rsyncwsrep_causal_reads=ON# Allow server to accept connections on all interfaces.#bind-address=192.168.200.11 #改IP，控制远程访问的 5、都登陆数据库并授权访问（node1、node2、node3）12345[root@node1 ~]# mysql -uroot -p000000MariaDB [(none)]&gt; grant all privileges on *.* to root@&#x27;%&#x27; identified by &#x27;000000&#x27;;MariaDB [(none)]&gt;exit#node2、node3操作同上 6、 将 3 个节点的数据库服务关闭123[root@node1 ~]# systemctl stop mariadb[root@node2 ~]# systemctl stop mariadb[root@node3 ~]# systemctl stop mariadb 7、node1 节点启动MariaDB Galera Cluster 服务1[root@node1 ~]# galera_new_cluster 8、node2 和 node3 节点启动数据库服务1[root@node2 ~]# systemctl start mariadb[root@node3 ~]# systemctl start mariadb 9、node1 节点，使用命令查看端口1##若看到 3306 和 4567 端口，则 MariaDB Galera Cluster集群配置成功[root@node1 ~]# ss -tnl[root@node1 ~]# netstat -ntpl #需要yum install -y net-tools //和ss -tnl一样 10、node1 节点，登录数据库，查看 Galera 插件1##登录数据库[root@node1 ~]# mysql -uroot -p000000 ①查看状态，为“ON” 1MariaDB [(none)]&gt; show status like &quot;wsrep_ready&quot;;+---------------+-------+| Variable_name | Value |+---------------+-------+| wsrep_ready | ON |+---------------+-------+1 row in set (0.001 sec)##或者执行[root@node1 ~]# mysql -uroot -p000000 -e &quot;SHOW STATUS LIKE &#x27;wsrep_ready&#x27;&quot; ②查看集群成员数量（加入集群节点数） 1MariaDB [(none)]&gt; show status like &quot;wsrep_cluster_size&quot;;+--------------------+-------+| Variable_name | Value |+--------------------+-------+| wsrep_cluster_size | 3 |+--------------------+-------+1 row in set (0.001 sec)##或者执行[root@node1 ~]# mysql -uroot -p000000 -e &quot;SHOW STATUS LIKE &#x27;wsrep_cluster_size&#x27;&quot; ③查看Galera集群状态 1MariaDB [(none)]&gt; show status like &quot;wsrep%&quot;;+-------------------------------+-------------------------------------------------------------+| Variable_name | Value |+-------------------------------+-------------------------------------------------------------+| wsrep_applier_thread_count | 1 || wsrep_apply_oooe | 0.000000 || wsrep_apply_oool | 0.000000 || wsrep_apply_window | 0.000000 || wsrep_causal_reads | 1 || wsrep_cert_deps_distance | 0.000000 || wsrep_cert_index_size | 0 || wsrep_cert_interval | 0.000000 || wsrep_cluster_conf_id | 3 || wsrep_cluster_size | 3 || wsrep_cluster_state_uuid | f6fff20a-ebbb-11eb-afce-47e2dba0b6f7 || wsrep_cluster_status | Primary || wsrep_cluster_weight | 3 || wsrep_commit_oooe | 0.000000 || wsrep_commit_oool | 0.000000 || wsrep_commit_window | 0.000000 || wsrep_connected | ON || wsrep_desync_count | 0 || wsrep_evs_delayed | || wsrep_evs_evict_list | || wsrep_evs_repl_latency | 0/0/0/0/0 || wsrep_evs_state | OPERATIONAL || wsrep_flow_control_paused | 0.000000 || wsrep_flow_control_paused_ns | 0 || wsrep_flow_control_recv | 0 || wsrep_flow_control_sent | 0 || wsrep_gcomm_uuid | f6ff67c4-ebbb-11eb-b48a-5b965989457c || wsrep_incoming_addresses | 192.168.200.12:3306,192.168.200.13:3306,192.168.200.11:3306 || wsrep_last_committed | 0 || wsrep_local_bf_aborts | 0 || wsrep_local_cached_downto | 18446744073709551615 || wsrep_local_cert_failures | 0 || wsrep_local_commits | 0 || wsrep_local_index | 2 || wsrep_local_recv_queue | 0 || wsrep_local_recv_queue_avg | 0.142857 || wsrep_local_recv_queue_max | 2 || wsrep_local_recv_queue_min | 0 || wsrep_local_replays | 0 || wsrep_local_send_queue | 0 || wsrep_local_send_queue_avg | 0.000000 || wsrep_local_send_queue_max | 1 || wsrep_local_send_queue_min | 0 || wsrep_local_state | 4 || wsrep_local_state_comment | Synced || wsrep_local_state_uuid | f6fff20a-ebbb-11eb-afce-47e2dba0b6f7 || wsrep_open_connections | 0 || wsrep_open_transactions | 0 || wsrep_protocol_version | 9 || wsrep_provider_name | Galera || wsrep_provider_vendor | Codership Oy &lt;info@codership.com&gt; || wsrep_provider_version | 25.3.28(r3875) || wsrep_ready | ON || wsrep_received | 7 || wsrep_received_bytes | 692 || wsrep_repl_data_bytes | 0 || wsrep_repl_keys | 0 || wsrep_repl_keys_bytes | 0 || wsrep_repl_other_bytes | 0 || wsrep_replicated | 0 || wsrep_replicated_bytes | 0 || wsrep_rollbacker_thread_count | 1 || wsrep_thread_count | 2 |+-------------------------------+-------------------------------------------------------------+63 rows in set (0.001 sec) ④看连接的主机 1MariaDB [(none)]&gt; show status like &quot;wsrep_incoming_addresses&quot;;+--------------------------+-------------------------------------------------------------+| Variable_name | Value |+--------------------------+-------------------------------------------------------------+| wsrep_incoming_addresses | 192.168.200.12:3306,192.168.200.13:3306,192.168.200.11:3306 |+--------------------------+-------------------------------------------------------------+1 row in set (0.001 sec)##或者执行[root@node1 ~]# mysql -uroot -p000000 -e &quot;SHOW STATUS LIKE &#x27;wsrep%&#x27;&quot; 11、node1节点上创建测试数据库testnode1、验证数据库集群的同步功能1MariaDB [(none)]&gt; create database testnode1;Query OK, 1 row affected (0.006 sec)MariaDB [(none)]&gt; show databases;+--------------------+| Database |+--------------------+| information_schema || mysql || performance_schema || testnode1 |+--------------------+4 rows in set (0.000 sec) 12、node2、node3节点查看同步情况（testnode1） 1mysql -uroot -p000000show databases;或者root@node2 ~]# mysql -uroot -p000000 -e &quot;SHOW DATABASES&quot;root@node3 ~]# mysql -uroot -p000000 -e &quot;SHOW DATABASES&quot; 13、node2节点创建数据库，node1、node3测试1##node2create database testnode1;show databases;##node1、node3show databases; 排错：1、错误汇总 https://www.cnblogs.com/zhongguiyao/p/14149654.html 2、集群无法正常启动 mariadb galera cluster集群故障恢复时，经常会遇到节点无法启动的情况，启动服务时报错： 1systemctl start mariadbJob for mariadb.service failed because the control process exited with error code. See &quot;systemctl status mariadb.service&quot; and &quot;journalctl -xe&quot; for details.1.2. 如果集群中还有存活的节点，那么离线的节点只需要执行systemctl start mariadb即可重新加入集群 如果所有节点均已离线，就会出现以上这种情况，此时需要人工确定启动顺序，先检查每个节点的/var/lib/mysql/grastate.dat文件，以测试环境为例，当前两节点上的mariadb服务均处于停止状态，grastate.dat的内容分别为： 1# GALERA saved stateversion: 2.1uuid: 44f8dbe5-1271-11eb-8206-1e1a48859dc8seqno: 157035safe_to_bootstrap: 01.2.3.4.5.# GALERA saved stateversion: 2.1uuid: 44f8dbe5-1271-11eb-8206-1e1a48859dc8seqno: 157036safe_to_bootstrap: 11.2.3.4.5. 可以看到两者有相同的uuid，但seqno和safe_to_bootstrap不同。集群中seqno最大的节点是优先启动节点，一般它的safe_to_bootstrap=1。此时以galera_new_cluster方式启动优先节点，然后再以systemctl start mariadb方式启动其它节点，集群就顺利恢复了。","categories":[],"tags":[]},{"title":"YAML语法学习","slug":"YAML语法学习","date":"2022-01-19T02:00:58.000Z","updated":"2022-02-22T10:18:39.553Z","comments":true,"path":"2022/01/19/YAML语法学习/","link":"","permalink":"http://example.com/2022/01/19/YAML%E8%AF%AD%E6%B3%95%E5%AD%A6%E4%B9%A0/","excerpt":"YAML语法学习1.简介YAML = YAML Ain’t Markup Language, YAML 是一种简洁的非标记语言。YAML以数据为中心，使用空白，缩进，分行组织数据，从而使得表示更加简洁易读。多用于api接口的定义。","text":"YAML语法学习1.简介YAML = YAML Ain’t Markup Language, YAML 是一种简洁的非标记语言。YAML以数据为中心，使用空白，缩进，分行组织数据，从而使得表示更加简洁易读。多用于api接口的定义。 2.基本规则1、大小写敏感2、使用缩进表示层级关系3、禁止使用tab缩进，只能使用空格键4、缩进长度没有限制，只要元素对齐就表示这些元素属于一个层级5、使用#表示注释6、字符串可以不用引号标注 3. YAML支持的数据结构对象：键值对的集合，又称为映射（map）/ 哈希（hashes） / 字典（dictionary） 1234567使用冒号（：）表示键值对，同一缩进的所有键值对属于一个map，示例：# YAML表示age : 12name : huang # 对应的Json表示&#123;&#x27;age&#x27;:12,&#x27;name&#x27;:&#x27;huang&#x27;&#125; 数组：一组按次序排列的值，又称为序列（sequence） / 列表（list） 1234567使用连字符（-）表示：# YAML表示- a- b- 12# 对应Json表示[&#x27;a&#x27;,&#x27;b&#x27;,12] 纯量（scalars）：单个的、不可再分的值 4.数据的嵌套1.map嵌套map 123456789101112# YAML表示websites: YAML: yaml.org Ruby: ruby-lang.org Python: python.org Perl: use.perl.org # 对应Json表示&#123; websites: &#123; YAML: &#x27;yaml.org&#x27;, Ruby: &#x27;ruby-lang.org&#x27;, Python: &#x27;python.org&#x27;, Perl: &#x27;use.perl.org&#x27; &#125; &#125; 2.map嵌套list 12345678# YAML表示languages: - Ruby - Perl - Python - c# 对应Json表示&#123; languages: [ &#x27;Ruby&#x27;, &#x27;Perl&#x27;, &#x27;Python&#x27;, &#x27;c&#x27; ] &#125; 3.list嵌套list 1234567891011# YAML表示- - Ruby - Perl - Python - - c - c++ - java# 对应Json表示[ [ &#x27;Ruby&#x27;, &#x27;Perl&#x27;, &#x27;Python&#x27; ], [ &#x27;c&#x27;, &#x27;c++&#x27;, &#x27;java&#x27; ] ] 4.list 嵌套map 12345678910# YAML表示- id: 1 name: huang- id: 2 name: liao # 对应Json表示[ &#123; id: 1, name: &#x27;huang&#x27; &#125;, &#123; id: 2, name: &#x27;liao&#x27; &#125; ] 5.其他1.引号单引号： 会将字符串里面的特殊字符转义为字符串处理 12name: &#x27;123\\n123&#x27;输出： 123\\n123 双引号： 不会转义字符串里面的特殊字符，特殊字符作为本身想表示的意思。 12name: &quot;123\\n123&quot; 输出： 123 换行 123 2.文本块|：使用|标注的文本内容缩进表示的块，可以保留块中已有的回车换行 1234value: | hello world!输出：hello 换行 world！ +表示保留文字块末尾的换行，-表示删除字符串末尾的换行。 123456789value: |hellovalue: |-hellovalue: |+hello输出：hello\\n hello hello\\n\\n(有多少个回车就有多少个\\n) ：使用 &gt; 标注的文本内容缩进表示的块，将块中回车替换为空格，最终连接成一行 1234value: &gt; helloworld!输出：hello 空格 world！注意 “&gt;” 与 文本之间的空格 3.锚点与引用使用 &amp; 定义数据锚点（即要复制的数据），使用 * 引用锚点数据（即数据的复制目的地） 123456789name: &amp;a yamlbook: *abooks: - java - *a - python输出book： yaml 输出books：[java,yaml,python]注意*引用部分不能追加内容 4.存量，数据类型约定1.字符串使用”或”“或不使用引号2.布尔值true或false表示。3.数字 12345612 #整数 014 # 八进制整数 0xC ＃十六进制整数 13.4 ＃浮点数 1.2e+34 ＃指数 .inf空值 ＃无穷大 4.空值null或~表示5.日期使用 iso-8601 标准表示日期 12date: 2018-01-01t16:59:43.10-05:00在springboot中yaml文件的时间格式 date: yyyy/MM/dd HH:mm:ss 6.强制类型转换YAML 允许使用个感叹号!，强制转换数据类型，单叹号通常是自定义类型，双叹号是内置类型。 123456789101112131415money: !!str123date: !Booleantrue内置类型：!!int # 整数类型 !!float # 浮点类型 !!bool # 布尔类型 !!str # 字符串类型 !!binary # 也是字符串类型 !!timestamp # 日期时间类型 !!null # 空值 !!set # 集合 !!omap,!!pairs # 键值列表或对象列表!!seq # 序列，也是列表 !!map # 键值表","categories":[],"tags":[]},{"title":"gluster-存储集群","slug":"gluster-存储集群","date":"2022-01-18T12:53:29.000Z","updated":"2022-02-22T10:18:15.711Z","comments":true,"path":"2022/01/18/gluster-存储集群/","link":"","permalink":"http://example.com/2022/01/18/gluster-%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4/","excerpt":"glusterfs-存储集群1 构建glusterfs分布式文件系统1.1实验目的1、了解glusterfs架构原理及其组件功能 2、掌握glusterfs文件系统的安装、配置和集群管理 3、熟练掌握glusterfs文件系统的挂载","text":"glusterfs-存储集群1 构建glusterfs分布式文件系统1.1实验目的1、了解glusterfs架构原理及其组件功能 2、掌握glusterfs文件系统的安装、配置和集群管理 3、熟练掌握glusterfs文件系统的挂载 2 实验内容1、在两个节点添加硬盘/dev/sdb，将sdb分区后充当glusterfs文件系统的底层存储（brick）； 2、安装、配置、启动glusterfs集群服务 3、创建复制卷 4、客户端挂载glusterfs卷 3 实验步骤3.1 创建brick（两台）1、关机 添加20G磁盘（/dev/sdb） 2、新建10GB逻辑卷（/dev/vg0/lv0），文件系统类型为xfs，挂载到/cluster目录 123456789101112# lsblk# pvcreate /dev/sdb# pvs# vgcreate vg0 /dev/sdb# vgs# lvcreate -n lv0 -L 10G vg0# lvs# mkfs.xfs /dev/vg0/lv0# mkdir /cluster# mount /dev/vg0/lv0 /cluster/# df -hT 3、创建brick（存储目录/cluster/gfs-brick） 1# mkdir /cluster/gfs-brick0 3.3.2 基础配置（两个节点）1、设置主机名 1234567//node1# hostnamectl set-hostname node1# bash//node2# hostnamectl set-hostname node2# bash 2、配置网络 12345678910111213141516171819202122232425//node1# vi /etc/sysconfig/network-scripts/ifcfg-ens33 TYPE=&quot;Ethernet&quot;BOOTPROTO=&quot;none&quot;NAME=&quot;ens33&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;IPADDR=&quot;192.168.200.11&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;192.168.200.2&quot;DNS1=&quot;114.114.114.114&quot;# systemctl restart network//node2# vi /etc/sysconfig/network-scripts/ifcfg-ens33TYPE=&quot;Ethernet&quot;BOOTPROTO=&quot;none&quot;NAME=&quot;ens33&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;IPADDR=&quot;192.168.200.12&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;192.168.200.2&quot;DNS1=&quot;114.114.114.114&quot;# systemctl restart network 3、修改/etc/hosts文件（两台） 123# vi /etc/hosts192.168.200.11 node1192.168.200.12 node2 4、关闭firewall防火墙、selinux、iptables（两个节点） 123456789101112# systemctl stop firewalld# setenforce 0# iptables -F# iptables -X# iptables -Z# iptables-save//说明：-F 清除链中所有规则 -X 清空自定义链的规则 -Z 清空计数器 -L 显示表中的规则 5、配置时间服务器 3.2 安装glusterfs（两个节点）1、添加glusterfs源 1# yum -y install centos-release-gluster 2、安装、启动glusterfs 1234# yum install -y glusterfs-server# systemctl enable glusterd &amp;&amp; systemctl start glusterd# systemctl status glusterd# bash //可以使用glusterfs的命令补全 3.3 管理集群1、添加节点到集群（node1） 12# gluster peer probe 192.168.200.12peer probe: success. 2、查询集群状态（node1） 123456# gluster peer status Number of Peers: 1Hostname: 192.168.200.12Uuid: b76ceed2-9c7f-4152-9579-86a9354dce0fState: Peer in Cluster (Connected) 3、创建glusterfs复制卷 1234567891011121314151617181920212223242526# gluster volume create gv0 replica 2 192.168.200.11:/cluster/gfs-brick0 192.168.200.12:/cluster/gfs-brick0 forceReplica 2 volumes are prone to split-brain. Use Arbiter or Replica 3 to avoid this. See: http://docs.gluster.org/en/latest/Administrator%20Guide/Split%20brain%20and%20ways%20to%20deal%20with%20it/.Do you still want to continue? (y/n) yvolume create: gv0: success: please start the volume to access data# gluster volume （tab tab）# gluster volume start gv0# gluster volume info Volume Name: gv0Type: ReplicateVolume ID: e95a6739-9a21-4d86-86ae-f5d4f872fd06Status: StartedSnapshot Count: 0Number of Bricks: 1 x 2 = 2Transport-type: tcpBricks:Brick1: 192.168.200.10:/share/brick1/lv0Brick2: 192.168.200.20:/share/brick1/lv0Options Reconfigured:transport.address-family: inetstorage.fips-mode-rchecksum: onnfs.disable: onperformance.client-io-threads: off 5、安装glusterfs客户端，挂载glusterfs文件系统（node0） 12345# yum install -y glusterfs-client# mkdir /opt/gfs-test# # mount -t glusterfs 192.168.200.11:/gv0 /opt/gfs-test/[root@node2 ~]# df -hT | grep gv0192.168.200.11:/gv0 fuse.glusterfs 10G 135M 9.9G 2% /opt/gfs-test 6、验证复制卷 1234//node0# touch /opt/gfs-test/a.txt//node1、node2# ls /cluster/gfs-brick0","categories":[],"tags":[]},{"title":"ceph-部署","slug":"ceph-部署手册","date":"2022-01-17T02:42:50.000Z","updated":"2022-02-22T10:12:09.915Z","comments":true,"path":"2022/01/17/ceph-部署手册/","link":"","permalink":"http://example.com/2022/01/17/ceph-%E9%83%A8%E7%BD%B2%E6%89%8B%E5%86%8C/","excerpt":"ceph部署（虚拟机 centos 7 ） 高扩展性 高性能 ⽆无单点的分布式⽂文件存储系统 可⽤用于对象存储，块设备存储和⽂文件系统存储 读写速度快利利于共享","text":"ceph部署（虚拟机 centos 7 ） 高扩展性 高性能 ⽆无单点的分布式⽂文件存储系统 可⽤用于对象存储，块设备存储和⽂文件系统存储 读写速度快利利于共享 自上而下可将Ceph系统分为4层：1、基础存储系统RADOS（Reliable, Autonomic, Distributed Object Store，即可靠的、自动化的、分布式的对象存储）RADOS本身也是分布式存储系统，CEPH所有的存储功能都是基于RADOS实现,RADOS由大量的存储设备节点组成，每个节点拥有自己的硬件资源（CPU、内存、硬盘、网络），并运行着操作系统和文件系统。2、基础库LIBRADOSLIBRADOS对下层的RADOS提供的功能进行封装，向上层提供给API,以便直接使用RADOS，而不是整个Ceph进行调用。3、高层应用接口该层包括RADOS GW（RADOS Gateway）,RBD（Reliable Block Device）以及CEPH FS（Ceph File System）三部分组成。其中，RADOS GW是一个提供与Amazon S3和Swift兼容的RESTful API的gateway，以供相应的对象存储应用开发使用。RADOS GW提供的API抽象层次更高，但功能则不如librados强大。因此，开发者应针对自己的需求选择使用。RBD则提供了一个标准的块设备接口，常用于在虚拟化的场景下为虚拟机创建volume。目前，Red Hat已经将RBD驱动集成在KVM/QEMU中，以提高虚拟机访问性能。Ceph FS是一个POSIX兼容的分布式文件系统。由于还处在开发状态，因而Ceph官网并不推荐将其用于生产环境中4、应用层这一层就是不同场景下对于Ceph各个应用接口的各种应用方式，例如基于librados直接开发的对象存储应用，基于RADOS GW开发的对象存储应用，基于RBD实现的云硬盘等等。 1 拓扑结构业务网络、存储网络 https://docs.ceph.com/en/latest/install/manual-deployment/ 序号 主机名 public-network(ens33) cluster-network(ens36) 1 node1（mon、osd）（部署、客户端） 192.168.200.11 192.168.100.11 2 node2（mon、osd） 192.168.200.12 192.168.100.12 3 node3（mon、osd） 192.168.200.13 192.168.100.13 1.1 配置主机网络(三个节点)12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758##node1# vim /etc/sysconfig/network-scripts/ifcfg-ens33TYPE=&quot;Ethernet&quot;BOOTPROTO=&quot;none&quot;NAME=&quot;ens33&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;IPADDR=&quot;192.168.200.11&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;192.168.200.2&quot;DNS1=&quot;114.114.114.114&quot;# systemctl restart network# ping qq.com# cd /etc/sysconfig/network-scripts/# cp ifcfg-ens33 ifcfg-ens36# vim ifcfg-ens36TYPE=EthernetBOOTPROTO=staticNAME=ens36DEVICE=ens36ONBOOT=yesIPADDR=192.168.100.11PREFIX=24# nmcli con reload# nmcli con up ens36# nmcli con show# nmcli con delete Wired\\ connection\\ 1 //使用W+Tab补全##node2# vim /etc/sysconfig/network-scripts/ifcfg-ens33TYPE=&quot;Ethernet&quot;BOOTPROTO=&quot;none&quot;NAME=&quot;ens33&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;IPADDR=&quot;192.168.200.12&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;192.168.200.2&quot;DNS1=&quot;114.114.114.114&quot;# systemctl restart network# ping qq.com##node3# vim /etc/sysconfig/network-scripts/ifcfg-ens33TYPE=&quot;Ethernet&quot;BOOTPROTO=&quot;none&quot;NAME=&quot;ens33&quot;DEVICE=&quot;ens33&quot;ONBOOT=&quot;yes&quot;IPADDR=&quot;192.168.200.13&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;192.168.200.2&quot;DNS1=&quot;114.114.114.114&quot;# systemctl restart network# ping qq.com 2 基础环境准备2.1 主机名和hosts设置（三个节点）123456789101112131415161718192021# hostnamectl set-hostname node1# bash# hostnamectl set-hostname node2# bash# hostnamectl set-hostname node3# bash[root@node1 ~]# vim /etc/hosts192.168.200.11 node1192.168.200.12 node2192.168.200.13 node3#拷贝hosts文件到node2、node3节点[root@node1 ~]# scp /etc/hosts node2:/etc[root@node1 ~]# scp /etc/hosts node3:/etc##分别在node2、node3节点查看hosts文件，是否正确[root@node2 ~]# cat /etc/hosts[root@node3 ~]# cat /etc/hosts 2.2 设置ssh无密码登录（部署节点-node1）123456789101112[root@node1 ~]# ssh-keygen #一路回车##传输公钥[root@node1 ~]# ssh-copy-id root@node1[root@node1 ~]# ssh-copy-id root@node2[root@node1 ~]# ssh-copy-id root@node3##ssh登录测试（可以正常登录）[root@node1 ~]# ssh node1[root@node1 ~]# exit[root@node1 ~]# ssh node2[root@node2 ~]# exit[root@node1 ~]# ssh node3[root@node3 ~]# exit 2.3 安全设置（三个节点）1、selinux 1234[root@node1 ~]# vim /etc/selinux/configSELINUX=disabled #第7行[root@node1 ~]# setenforce 0 #临时生效# getenforce 2、firewalld 123[root@node1 ~]# systemctl disable firewalld &amp;&amp; systemctl stop firewalld[root@node1 ~]# firewall-cmd --list-allFirewallD is not running 2.4 ntp时间同步（三个节点）1234567# vim /etc/chrony.conf#注释掉3-6行，新添加第7行server 120.25.115.20 iburst #第7行# systemctl restart chronyd# chronyc -a makestep #立即手工同步200 OK# chronyc sources -v #查看时间同步 2.5 yum环境源（三个节点）国内阿里、163、清华 123456789101112131415161718192021222324252627282930# mv -f /etc/yum.repos.d/* /media/# wget -O /etc/yum.repos.d/CentOS-Base.repo https://mirrors.huaweicloud.com/repository/conf/CentOS-7-anon.repo# cat &gt;&gt; /etc/yum.repos.d/ceph.repo &lt;&lt; EOF[ceph]name=ceph_huaweicloudbaseurl=https://mirrors.huaweicloud.com/ceph/rpm-15.2.4/el7/x86_64/gpgcheck=0enabled=1[ceph-noarch]name=ceph-noarch_huaweicloudbaseurl=https://mirrors.huaweicloud.com/ceph/rpm-15.2.4/el7/noarch/gpgcheck=0enabled=1EOF# cat &gt;&gt; /etc/yum.repos.d/epel.repo &lt;&lt; EOF[epel]name=epel_huaweicloudbaseurl=https://mirrors.huaweicloud.com/epel/7/x86_64/gpgcheck=0enabled=1EOF# ls /etc/yum.repos.d/# scp /etc/yum.repos.d/* node2:/etc/yum.repos.d/# scp /etc/yum.repos.d/* node3:/etc/yum.repos.d/ 3 部署ceph集群3.1 安装ceph-deploy（node1）1234567[root@node1 yum.repos.d]# yum install -y python-setuptools python3[root@node1 yum.repos.d]# yum install -y ceph-deploy[root@node1 yum.repos.d]# ceph-deploy --version##补足ceph缺失库pip3 install pecanpip3 install werkzeug 3.2 部署mon1[root@node1 ~]# mkdir ~/my-cluster &amp;&amp; cd my-cluster/[root@node1 my-cluster]# ceph-deploy new node1# cat &gt;&gt; ceph.conf &lt;&lt;EOFpublic_network=192.168.200.0/24EOF[root@node1 my-cluster]# ls#在3个节点（node1、node2、node3）安装ceph软件包，本地yum（可以指定本地yum，也可以各个节点单独安装）[root@node1 my-cluster]# yum install -y ceph ceph-mon ceph-osd ceph-mds ceph-radosgw ceph-mgr[root@node2 ~]# yum install -y ceph ceph-mon ceph-osd ceph-mds ceph-radosgw ceph-mgr[root@node3 ~]# yum install -y ceph ceph-mon ceph-osd ceph-mds ceph-radosgw ceph-mgr##初始化mon[root@node1 my-cluster]# ceph-deploy mon create-initial[root@node1 my-cluster]# ls[root@node1 my-cluster]# ceph-deploy admin node1 node2 node3 #推送配置文件[root@node1 my-cluster]# ceph -s========考试提交（截图）========[root@node1 my-cluster]# ceph -s cluster: id: 1d6f6297-1f28-400c-b935-81f5d011486c health: HEALTH_OK services: mon: 1 daemons, quorum node1 (age 77s) mgr: no daemons active osd: 0 osds: 0 up, 0 in data: pools: 0 pools, 0 pgs objects: 0 objects, 0 B usage: 0 B used, 0 B / 0 B avail pgs: 3.3部署mgr1[root@node1 my-cluster]# ceph-deploy mgr create node1[root@node1 my-cluster]# ceph -s 3.4 部署osd1[root@node1 my-cluster]# ceph-deploy osd create --data /dev/sdb node1[root@node1 my-cluster]# ceph-deploy osd create --data /dev/sdb node2[root@node1 my-cluster]# ceph-deploy osd create --data /dev/sdb node3[root@node1 my-cluster]# ceph -s[root@node1 my-cluster]# ceph osd status[root@node1 my-cluster]# ceph osd tree========考试提交（截图）========[root@node1 my-cluster]# ceph osd treeID CLASS WEIGHT TYPE NAME STATUS REWEIGHT PRI-AFF-1 0.05846 root default -3 0.01949 host node1 0 hdd 0.01949 osd.0 up 1.00000 1.00000-5 0.01949 host node2 1 hdd 0.01949 osd.1 up 1.00000 1.00000-7 0.01949 host node3 2 hdd 0.01949 osd.2 up 1.00000 1.00000 3.5 扩展mon和mgr1##扩展mon[root@node1 my-cluster]# ceph-deploy mon add node2[root@node1 my-cluster]# ceph-deploy mon add node3[root@node1 my-cluster]# ceph -s[root@node1 my-cluster]# ceph mon dump========考试提交（截图）========[root@node1 my-cluster]# ceph mon dumpdumped monmap epoch 3epoch 3fsid 1d6f6297-1f28-400c-b935-81f5d011486clast_changed 2021-12-04T09:23:40.570962+0800created 2021-12-04T09:18:22.391261+0800min_mon_release 15 (octopus)0: [v2:192.168.200.11:3300/0,v1:192.168.200.11:6789/0] mon.node11: [v2:192.168.200.12:3300/0,v1:192.168.200.12:6789/0] mon.node22: [v2:192.168.200.13:3300/0,v1:192.168.200.13:6789/0] mon.node3##扩展mgr[root@node1 my-cluster]# ceph-deploy mgr create node2 node3[root@node1 my-cluster]# ceph -s 3.6 ceph集群的网络将ceph集群的网络分离出public network和cluster network执行以下命令，验证osd是否监听两个网络平面，并截图（所有节点都需要截图） 1##添加一块仅主机网卡：网段192.168.100.0/24，三个节点先配置网络。[root@node1 my-cluster]# cat &gt;&gt; ceph.conf &lt;&lt;EOF&gt; cluster_network=192.168.100.0/24&gt; EOF //单个粘# cat ceph.conf[root@node1 my-cluster]# ceph-deploy --overwrite-conf config push node1 node2 node3//node1-3都执行# systemctl restart ceph.target# ss -ntlp | grep ceph-osd (三个节点都需要认证)========考试提交（截图）========# ss -ntl | grep ceph-osd[root@node1 my-cluster]# ss -ntlp | grep ceph-osdLISTEN 0 128 192.168.100.11:6800 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=19))LISTEN 0 128 192.168.200.11:6800 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=17))LISTEN 0 128 192.168.100.11:6801 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=20))LISTEN 0 128 192.168.200.11:6801 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=18))LISTEN 0 128 192.168.100.11:6802 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=23))LISTEN 0 128 192.168.200.11:6802 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=21))LISTEN 0 128 192.168.100.11:6803 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=24))LISTEN 0 128 192.168.200.11:6803 *:* users:((&quot;ceph-osd&quot;,pid=14198,fd=22)) 3.7 创建一个存储池存储池的名称为testpoolXX（XX为本人学号后两位） 执行以下命令，查看存储池列表，并截图 1[root@node1 my-cluster]# ceph osd pool create testpool99pool &#x27;testpool99&#x27; created# ceph osd pool ls========考试提交（截图）========[root@node1 my-cluster]# ceph osd pool lsdevice_health_metricstestpool99 3.8 上传一个对象到testpoolXX存储池，对象的名称为testobjectXX（XXXX为本人学号后两位）1[root@node1 my-cluster]# touch testobject99[root@node1 my-cluster]# rados -p testpool99 put testobject99 testobject99========考试提交（截图）========[root@node1 my-cluster]# rados ls -p testpool99testobject99 3.9 8. 创建一个RBD的image，image名称为testimageXXXX，大小为100M（XXXX为本人学号后两位）1[root@node1 my-cluster]# rbd create testpool99/testimage99 --size 100[root@node1 my-cluster]# rbd info testpool99/testimage99========考试提交（截图）========[root@node1 my-cluster]# rbd info testpool99/testimage99rbd image &#x27;testimage99&#x27;: size 100 MiB in 25 objects order 22 (4 MiB objects) snapshot_count: 0 id: 85dd27767c81 block_name_prefix: rbd_data.85dd27767c81 format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Sat Dec 4 09:57:17 2021 access_timestamp: Sat Dec 4 09:57:17 2021 modify_timestamp: Sat Dec 4 09:57:17 2021 3.10 将testimageXXXX映射为块设备1[root@node1 my-cluster]# rbd feature disable testpool99/testimage99 object-map fast-diff deep-flatten[root@node1 my-cluster]# rbd device map testpool99/testimage99 ========考试提交（截图）========[root@node1 my-cluster]# rbd device map testpool99/testimage99 /dev/rbd0//补充，块使用[root@node1 my-cluster]# fdisk /dev/rbd0 n p 1 回车 回车 w[root@node1 my-cluster]# lsblk[root@node1 my-cluster]# mkfs.xfs /dev/rbd0p1Discarding blocks...Done.meta-data=/dev/rbd0p1 isize=512 agcount=4, agsize=6144 blks = sectsz=512 attr=2, projid32bit=1 = crc=1 finobt=0, sparse=0data = bsize=4096 blocks=24576, imaxpct=25 = sunit=1024 swidth=1024 blksnaming =version 2 bsize=4096 ascii-ci=0 ftype=1log =internal log bsize=4096 blocks=624, version=2 = sectsz=512 sunit=8 blks, lazy-count=1realtime =none extsz=4096 blocks=0, rtextents=0[root@node1 my-cluster]# mkdir /blk1[root@node1 my-cluster]# mount /dev/rbd0p1 /blk1/[root@node1 my-cluster]# df -hT | grep blk1/dev/rbd0p1 xfs 94M 5.1M 89M 6% /blk1[root@node1 my-cluster]# touch /blk1/a.txt 3.11 删除testpoolXXXX存储池1# cat &gt;&gt; /etc/ceph/ceph.conf &lt;&lt;EOF[mon]mon allow pool delete = trueEOF# systemctl restart ceph-mon.target# ceph osd pool delete testpool99 testpool99 --yes-i-really-really-mean-it# ceph osd pool ls========考试提交（截图）========[root@node1 my-cluster]# ceph osd pool lsdevice_health_metrics 4 使用ceph4.1 配置管理块存储（rbd）https://blog.csdn.net/qq_36441027/article/details/81536603?depth_1- 1、创建资源池 1##查看池[root@node1 my-cluster]# ceph osd lspools [root@node1 my-cluster]# ceph osd pool ls##查看命令使用方法[root@node1 my-cluster]# ceph osd pool[root@node1 my-cluster]# ceph osd pool create##创建池[root@node1 my-cluster]# ceph osd pool create ceph-demo 64 64pool &#x27;ceph-demo&#x27; created[root@node1 my-cluster]# ceph osd lspools1 ceph-demo##查看[root@node1 my-cluster]# ceph osd pool get ceph-demo pg_numpg_num: 64[root@node1 my-cluster]# ceph osd pool get ceph-demo pgp_numpgp_num: 64[root@node1 my-cluster]# ceph osd pool get ceph-demo sizesize: 3##修改副本数为2[root@node1 my-cluster]# ceph osd pool set ceph-demo size 2set pool 1 size to 2[root@node1 my-cluster]# ceph osd pool get ceph-demo sizesize: 2#调整gp数到128，需要把pgp和pg调整为相同[root@node1 my-cluster]# ceph osd pool set ceph-demo pg_num 128set pool 1 pg_num to 128[root@node1 my-cluster]# ceph osd pool get ceph-demo pg_numpg_num: 128[root@node1 my-cluster]# ceph -s[root@node1 my-cluster]# ceph osd pool set ceph-demo pgp_num 128set pool 1 pgp_num to 128[root@node1 my-cluster]# ceph osd pool get ceph-demo pgp_numpgp_num: 128 2、rbd块存储创建和映射（rbd） 1##node1作为客户端[root@node1 ~]# rbd help create##两种写法[root@node1 ~]# rbd create --pool ceph-demo --image rbd-demo.img --size 2G[root@node1 ~]# rbd create ceph-demo/rbd-demo-1.img --size 1G##查看创建的块存储[root@node1 ~]# rbd ls -p ceph-demorbd-demo-1.imgrbd-demo.img##查看块存储信息，有features特性挂载时不支持，需要删除[root@node1 ~]# rbd info ceph-demo/rbd-demo.imgrbd image &#x27;rbd-demo.img&#x27;: size 2 GiB in 512 objects order 22 (4 MiB objects) snapshot_count: 0 id: 119d58e5c92a block_name_prefix: rbd_data.119d58e5c92a format: 2 features: layering, exclusive-lock, object-map, fast-diff, deep-flatten op_features: flags: create_timestamp: Thu Jan 28 13:01:43 2021 access_timestamp: Thu Jan 28 13:01:43 2021 modify_timestamp: Thu Jan 28 13:01:43 2021##删除[root@node1 ~]# rbd rm -p ceph-demo --image rbd-demo-1.imgRemoving image: 100% complete...done.[root@node1 ~]# rbd ls -p ceph-demorbd-demo.img##块设备的使用，一般是映射给虚拟机使用，这里通过map映射使用[root@node1 ~]# rbd map ceph-demo/rbd-demo.img #报错，有不支持项需要禁用[root@node1 ~]# rbd feature disable ceph-demo/rbd-demo.img object-map fast-diff deep-flatten##映射为块存储[root@node1 ~]# rbd map ceph-demo/rbd-demo.img /dev/rbd0##查看存储设备[root@node1 ~]# rbd device listid pool namespace image snap device 0 ceph-demo rbd-demo.img - /dev/rbd0 [root@node1 ~]# lsblk##格式化挂载使用[root@node1 ~]# mkfs.xfs /dev/rbd0[root@node1 ~]# mkdir /mnt/ceph-demo[root@node1 ~]# mount /dev/rbd0 /mnt/ceph-demo/[root@node1 ~]# df -h | grep /dev/rbd0/dev/rbd0 2.0G 33M 2.0G 2% /mnt/ceph-demo 3、rbd块存储扩容 1[root@node1 ~]# rbd resize ceph-demo/rbd-demo.img --size 3GResizing image: 100% complete...done.[root@node1 ~]# rbd info ceph-demo/rbd-demo.imgrbd image &#x27;rbd-demo.img&#x27;: size 3 GiB in 768 objects order 22 (4 MiB objects) snapshot_count: 0……##文件系统扩容[root@node1 ~]# xfs_growfs /dev/rbd0[root@node1 ~]# df -h | grep /dev/rbd0/dev/rbd0 3.0G 33M 3.0G 2% /mnt/ceph-demo 4.2 radosgw-对象存储网关https://www.cnblogs.com/zyxnhr/p/10562017.html https://cloud.tencent.com/developer/article/1664605 对象存储需要使用到radosgw，即对象存储网关，所以需要先起一个网关，在起网关的时候，它会检查它需要的存储池是否存在，不存在则自动创建，创建网关在deploy节点上进行 4.3 cephfshttps://blog.csdn.net/qq_36270681/article/details/108408163 https://blog.51cto.com/liujingyu/2568713 https://blog.csdn.net/m0_45036821/article/details/105944154","categories":[],"tags":[]},{"title":"杨宝宝亲启","slug":"杨宝宝亲启","date":"2022-01-16T14:30:26.000Z","updated":"2022-01-16T14:31:24.076Z","comments":true,"path":"2022/01/16/杨宝宝亲启/","link":"","permalink":"http://example.com/2022/01/16/%E6%9D%A8%E5%AE%9D%E5%AE%9D%E4%BA%B2%E5%90%AF/","excerpt":"","text":"爱你哦~ mua~ 嘿嘿~","categories":[],"tags":[]},{"title":"k8s-全面","slug":"k8s-全面","date":"2022-01-16T12:33:41.000Z","updated":"2022-02-22T10:16:57.762Z","comments":true,"path":"2022/01/16/k8s-全面/","link":"","permalink":"http://example.com/2022/01/16/k8s-%E5%85%A8%E9%9D%A2/","excerpt":"Kubernetes1 kubernetes-整体概述和架构详解1.1 概述Kubernetes是一个轻便的和可扩展的开源平台，用于管理容器化应用和服务。通过Kubernetes能够进行应用的自动化部署和扩缩容。在Kubernetes中，会将组成应用的容器组合成一个逻辑单元以更易管理和发现。Kubernetes积累了作为Google生产环境运行工作负载15年的经验，并吸收了来自于社区的最佳想法和实践。Kubernetes经过这几年的快速发展，形成了一个大的生态环境，Google在2014年将Kubernetes作为开源项目。","text":"Kubernetes1 kubernetes-整体概述和架构详解1.1 概述Kubernetes是一个轻便的和可扩展的开源平台，用于管理容器化应用和服务。通过Kubernetes能够进行应用的自动化部署和扩缩容。在Kubernetes中，会将组成应用的容器组合成一个逻辑单元以更易管理和发现。Kubernetes积累了作为Google生产环境运行工作负载15年的经验，并吸收了来自于社区的最佳想法和实践。Kubernetes经过这几年的快速发展，形成了一个大的生态环境，Google在2014年将Kubernetes作为开源项目。 Kubernetes的关键特性包括： 自动化装箱在不牺牲可用性的条件下，基于容器对资源的要求和约束自动部署容器。同时，为了提高利用率和节省更多资源，将关键和最佳工作量结合在一起。 自愈能力当容器失败时，会对容器进行重启；当所部署的Node节点有问题时，会对容器进行重新部署和重新调度；当容器未通过监控检查时，会关闭此容器；直到容器正常运行时，才会对外提供服务。 水平扩容通过简单的命令、用户界面或基于CPU的使用情况，能够对应用进行扩容和缩容。 服务发现和负载均衡：开发者不需要使用额外的服务发现机制，就能够基于Kubernetes进行服务发现和负载均衡。 自动发布和回滚Kubernetes能够程序化的发布应用和相关的配置。如果发布有问题，Kubernetes将能够回归发生的变更。 保密和配置管理在不需要重新构建镜像的情况下，可以部署和更新保密和应用配置。 存储编排自动挂接存储系统，这些存储系统可以来自于本地、公共云提供商（例如：GCP和AWS）、网络存储(例如：NFS、iSCSI、Gluster、Ceph、Cinder和Floker等)。 1.2 Kubernetes的整体架构 Kubernetes属于主从分布式架构，主要由Master Node和Worker Node组成，以及包括客户端命令行工具kubectl和其它附加项。 Master Node作为控制节点，对集群进行调度管理；Master Node由API Server、Scheduler、Cluster State Store（etcd）和Controller Manger Server所组成； Worker Node作为真正的工作节点，运行业务应用的容器；Worker Node包含kubelet、kube proxy和Container Runtime； kubectl用于通过命令行与API Server进行交互，而对Kubernetes进行操作，实现在集群中进行各种资源的增删改查等操作； Add-on是对Kubernetes核心功能的扩展，例如增加网络和网络策略等能力。 1.2.1 Master Node（主节点）1、API Server（API服务器） API Server主要用来处理REST的操作，确保它们生效，并执行相关业务逻辑，以及更新etcd（或者其他存储）中的相关对象。API Server是所有REST命令的入口，它的相关结果状态将被保存在etcd（或其他存储）中。API Server的基本功能包括： REST语义，监控，持久化和一致性保证，API 版本控制，放弃和生效 内置准入控制语义，同步准入控制钩子，以及异步资源初始化 API注册和发现 API Server也作为集群的网关默认情况，客户端通过API Server对集群进行访问，客户端需要通过认证，并使用API Server作为访问Node和Pod（以及service）的堡垒和代理/通道。 2、Cluster state store（集群状态存储） Kubernetes默认使用etcd作为集群整体存储，当然也可以使用其它的技术。etcd是一个简单的、分布式的、一致的key-value存储，主要被用来共享配置和服务发现。etcd提供了一个CRUD操作的REST API，以及提供了作为注册的接口，以监控指定的Node。集群的所有状态都存储在etcd实例中，并具有监控的能力，因此当etcd中的信息发生变化时，就能够快速的通知集群中相关的组件。 3、Controller-Manager Server（控制管理服务器） Controller-Manager Serve用于执行大部分的集群层次的功能，它既执行生命周期功能(例如：命名空间创建和生命周期、事件垃圾收集、已终止垃圾收集、级联删除垃圾收集、node垃圾收集)，也执行API业务逻辑（例如：pod的弹性扩容）。控制管理提供自愈能力、扩容、应用生命周期管理、服务发现、路由、服务绑定和提供。Kubernetes默认提供Replication Controller、Node Controller、Namespace Controller、Service Controller、Endpoints Controller、Persistent Controller、DaemonSet Controller等控制器。 4、Scheduler（调度器） scheduler组件为容器自动选择运行的主机。依据请求资源的可用性，服务请求的质量等约束条件，scheduler监控未绑定的pod，并将其绑定至特定的node节点。Kubernetes也支持用户自己提供的调度器，Scheduler负责根据调度策略自动将Pod部署到合适Node中，调度策略分为预选策略和优选策略，Pod的整个调度过程分为两步： 预选Node遍历集群中所有的Node，按照具体的预选策略筛选出符合要求的Node列表。如没有Node符合预选策略规则，该Pod就会被挂起，直到集群中出现符合要求的Node。 优选Node预选Node列表的基础上，按照优选策略为待选的Node进行打分和排序，从中获取最优Node。 1.2.2 Worker Node（从节点）1、Kubelet Kubelet是Kubernetes中最主要的控制器，维护容器的生命周期，并管理CSI（Container Storage Interface）和CNI（Conteinre Network Interface）。它是Pod和Node API的主要实现者，Kubelet负责驱动容器执行层。在Kubernetes中，应用容器彼此是隔离的，并且与运行其的主机也是隔离的，这是对应用进行独立解耦管理的关键点。 在Kubernets中，Pod作为基本的执行单元，它可以拥有多个容器和存储数据卷，能够方便在每个容器中打包一个单一的应用，从而解耦了应用构建时和部署时的所关心的事项，已经能够方便在物理机/虚拟机之间进行迁移。API准入控制可以拒绝或者Pod，或者为Pod添加额外的调度约束，但是Kubelet才是Pod是否能够运行在特定Node上的最终裁决者，而不是scheduler或者DaemonSet。kubelet默认情况使用cAdvisor进行资源监控。负责管理Pod、容器、镜像、数据卷等，实现集群对节点的管理，并将容器的运行状态汇报给Kubernetes API Server。 2、Container Runtime（容器运行时） 每一个Node都会运行一个Container Runtime，其负责下载镜像和运行容器。Kubernetes本身并不停容器运行时环境，但提供了接口，可以插入所选择的容器运行时环境。kubelet使用Unix socket之上的gRPC框架与容器运行时进行通信，kubelet作为客户端，而CRI shim作为服务器。 protocol buffers API提供两个gRPC服务，ImageService和RuntimeService。ImageService提供拉取、查看、和移除镜像的RPC。RuntimeSerivce则提供管理Pods和容器生命周期管理的RPC，以及与容器进行交互(exec/attach/port-forward)。容器运行时能够同时管理镜像和容器（例如：Docker和Rkt），并且可以通过同一个套接字提供这两种服务。在Kubelet中，这个套接字通过–container-runtime-endpoint和–image-service-endpoint字段进行设置。Kubernetes CRI支持的容器运行时包括docker、rkt、cri-o、frankti、kata-containers和clear-containers等。 3、kube proxy 基于一种公共访问策略（例如：负载均衡），服务提供了一种访问一群pod的途径。此方式通过创建一个虚拟的IP来实现，客户端能够访问此IP，并能够将服务透明的代理至Pod。每一个Node都会运行一个kube-proxy，kube proxy通过iptables规则引导访问至服务IP，并将重定向至正确的后端应用，通过这种方式kube-proxy提供了一个高可用的负载均衡解决方案。服务发现主要通过DNS实现。 在Kubernetes中，kube proxy负责为Pod创建代理服务；引到访问至服务；并实现服务到Pod的路由和转发，以及通过应用的负载均衡。 1.2.3 kubectlkubectl是Kubernetes集群的命令行接口。运行kubectl命令的语法如下所示 12345678910111213$ kubectl [command] [TYPE] [NAME] [flags]##这里的command，TYPE、NAME和flags为：##comand：指定要对资源执行的操作，例如create、get、describe和delete##TYPE：指定资源类型，资源类型是大小写敏感的，开发者能够以单数、复数和缩略的形式。例如：$ kubectl get pod pod1 $ kubectl get pods pod1 $ kubectl get po pod1##NAME：指定资源的名称，名称也大小写敏感的。如果省略名称，则会显示所有的资源，例如:$kubectl get pods##flags：指定可选的参数。例如，可以使用-s或者–server参数指定Kubernetes API server的地址和端口。##另外，可以通过运行kubectl help命令获取更多的信息。 7.1.2.4 add-one(附加项和其他依赖)在Kunbernetes中可以以附加项的方式扩展Kubernetes的功能，目前主要有网络、服务发现和可视化这三大类的附加项，下面是可用的一些附加项： 1、网络和网络策略 ACI 通过与Cisco ACI集成的容器网络和网络安全。 Calico[ˈkælɪkoʊ]， 是一个安全的3层网络和网络策略提供者。 Canal 联合Fannel和Calico，通过网络和网络侧。 Cilium 是一个3层网络和网络侧插件，它能够透明的加强HTTP/API/L7 策略。既支持路由，也支持overlay/encapsultion模式。 Flannel[ˈflænl] 是一个overlay的网络提供者。 2、服务发现 CoreDNS 是一个灵活的，可扩展的DNS服务器，它能够作为Pod集群内的DNS进行安装。 Ingress [ˈɪnɡres]提供基于Http协议的路由转发机制。 3、可视化&amp;控制 Dashboard 是Kubernetes的web用户界面。 小结 etcd 保存了整个集群的状态,服务注册发现； kube-apiserver 提供了资源操作的唯一入口，并提供认证、授权、访问控制等机制； kube-controller-manager 负责维护集群的状态，比如故障检测、自动扩展、滚动更新等； kube-scheduler 负责资源的调度，按照预定的调度策略将 Pod 调度到相应的机器上； kubelet 负责维持容器的生命周期，同时也负责 Volume（CVI）和网络（CNI）的管理； Container runtime 负责镜像管理以及 Pod 和容器的真正运行（CRI），默认的容器运行时为 Docker； kube-proxy 负责为 Service 提供 cluster 内部的服务发现和负载均衡； 7.1.3 kubenetes工作原理7.1.3.1工作原理 准备包含应用程序的Deployment的yml文件，然后通过kubectl客户端工具发送给ApiServer。 ApiServer接收到客户端的请求并将资源内容存储到数据库(etcd)中。 Controller组件(包括scheduler、replication、endpoint)监控资源变化并作出反应。 ReplicaSet检查数据库变化，创建期望数量的pod实例。 Scheduler再次检查数据库变化，发现尚未被分配到具体执行节点(node)的Pod，然后根据一组相关规则将pod分配到可以运行它们的节点上，并更新数据库，记录pod分配情况。 Kubelete监控数据库变化，管理后续pod的生命周期，发现被分配到它所在的节点上运行的那些pod。如果找到新pod，则会在该节点上运行这个新pod。 kuberproxy运行在集群各个主机上，管理网络通信，如服务发现、负载均衡。当有数据发送到主机时，将其路由到正确的pod或容器。对于从主机上发出的数据，它可以基于请求地址发现远程服务器，并将数据正确路由，在某些情况下会使用轮循调度算法(Round-robin)将请求发送到集群中的多个实例。 1.3.2 pod创建的时序图1、用户提交创建Pod的请求，可以通过API Server的REST API ，也可用Kubectl命令行工具，支持Json和Yaml两种格式； 2、API Server 处理用户请求，存储Pod数据到Etcd； 3、Schedule通过和 API Server的watch机制，查看到新的pod，尝试为Pod绑定Node； 4、过滤主机：调度器用一组规则过滤掉不符合要求的主机（比如Pod指定了所需要的资源，那么就要过滤掉资源不够的主机）； 5、主机打分：对第一步筛选出的符合要求的主机进行打分，在主机打分阶段，调度器会考虑一些整体优化策略。比如把一个Replication Controller的副本分布到不同的主机上，使用最低负载的主机等； 6、选择主机：选择打分最高的主机，进行binding操作，结果存储到Etcd中； 7、kubelet根据调度结果执行Pod创建操作： （1）绑定成功后，会启动container, docker run, （2）scheduler会调用API Server的API在etcd中创建一个bound pod对象，描述在一个工作节点上绑定运行的所有pod信息。 （3）运行在每个工作节点上的kubelet也会定期与etcd同步bound pod信息 （4）一旦发现应该在该工作节点上运行的bound pod对象没有更新，则调用Docker API创建并启动pod内的容器。 2 部署Kubernetes集群（kubeadm）2.1 实验准备1、实验环境（克隆三台裸机-未配置） 主机名 IP地址 节点角色 master 192.168.200.10 管理节点（2C、2G） node1 192.168.200.11 工作节点（2C、2G） node2 192.168.200.12 工作节点（2C、2G） 2、基础配置 （1）设置主机名（3台） 1234567891011*//master# hostnamectl set-hostname master# bash//node1# hostnamectl set-hostname node1# bash//node2# hostnamectl set-hostname node2# bash （2）配置主机网络（3台） 123456789101112131415161718# vi /etc/sysconfig/network-scripts/ifcfg-ens33TYPE=EthernetBOOTPROTO=staticNAME=ens33DEVICE=ens33ONBOOT=yesIPADDR=192.168.200.10 #master：10，node1：11，node2：12PROFIX=24GATEWAY=192.168.200.2DNS1=114.114.114.114# systemctl restart network# ip addr list ens33 # ping www.qq.com -c 2//验证各节点mac的UUID，不能相同（克隆会自动修改）# cat /sys/class/net/ens33/address# cat /sys/class/dmi/id/product_uuid （3）xshell（/MobaXterm）远程连接、安装常用软件 1234567# yum install -y vim bash-completion wget unzip net-tools# bash========命令补全（重要）========yum -y install bash-completionsource /etc/profile.d/bash_completion.sh============================= （4）免密登录（master） 12345678910111213141516171819202122232425262728*//master，编辑/etc/hosts文件（DNS劫持）# cat &lt;&lt;EOF &gt;&gt;/etc/hosts192.168.200.10 master192.168.200.11 node1192.168.200.12 node2EOF//查看配置文件# cat /etc/hostscat /etc/hosts//配置免密登录# ssh-keygen #一路回车# ssh-copy-id node1 #yes--000000# ssh-copy-id node2 #yes--000000========主机较多时也可以通过for循环操作（不操作）========ssh-keygenfor i in master node1 node2dossh-copy-id $i done//思考：怎样实现免交互=================================================== //拷贝/etc/hosts文件到node1、node2# scp /etc/hosts node1:/etc# scp /etc/hosts node2:/etc （5）关闭防火墙和selinux（/iptables）（3台） 12345678# systemctl stop firewalld &amp;&amp; systemctl disable firewalld# sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/&#x27; /etc/selinux/config# grep SELINUX=disabled /etc/selinux/config# setenforce 0//清空iptables规则# iptables -F # iptables-save （6）关闭swap分区 12# swapoff -a #临时关闭# sed -ri &#x27;s/.*swap.* /#&amp;/&#x27; /etc/fstab #永久关闭 （7）配置系统内核参数，添加网桥过滤及地址转发（3台） 12345678910111213141516171819*# cat &gt; /etc/sysctl.d/k8s.conf &lt;&lt;EOFnet.bridge.bridge-nf-call-ip6tables = 1net.bridge.bridge-nf-call-iptables = 1net.ipv4.ip_forward = 1vm.swappiness = 0EOF//向内核中加载模块# modprobe br_netfilter//手动加载所有的配置文件# sysctl --system//单独加载配置文件（和上面命令二选一）# sysctl -p /etc/sysctl.d/k8s.conf========其他内核模块配置--扩展阅读===========https://blog.51cto.com/8999a/2784601========================================= （8）配置时间服务器 1//修改时区# timedatectl ##查看时区# timedatectl set-timezone Asia/Shanghai# reboot//修改时间服务器为阿里云时间服务器# yum install -y chrony# systemctl enable chronyd &amp;&amp; systemctl start chronyd# vi /etc/chrony.conf……server ntp1.aliyun.com iburst #修改server行（3-7行）server ntp2.aliyun.com iburstserver ntp3.aliyun.com iburstserver ntp4.aliyun.com iburst……# systemctl restart chronyd//查看时间同步状态# timedatectl status# chronyc sources -c//开启网络时间同步# timedatectl set-ntp true//强制同步时钟# chronyc -a makestep 3、配置yum仓库 1*//备份原有的源# mkdir /opt/yum-bak# mv /etc/yum.repos.d/* /opt/yum-bak//下载基础源（阿里）# curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo//下载epel源# curl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo//配置docker-ce源# curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo//配置Kubernets源# cat &lt;&lt;EOF &gt; /etc/yum.repos.d/kubernetes.repo[kubernetes]name=Kubernetesbaseurl=https://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64/gpgcheck=0EOF//查看yum仓库# yum clean all# yum repolist========yum仓库说明========实际使用中根据下载速度选择清华、华为、中科大或163等源，具体操作自行百度========================== 2.2 安装Docker-ce（3台）1、安装docker-ce 1*# yum list --showduplicates docker-ce #列出可用版本# yum install -y yum-utils device-mapper-persistent-data lvm2# yum install -y docker-ce//启动docker# systemctl enable docker &amp;&amp; systemctl start docker//查看# systemctl status docker# docker info 2、配置docker镜像加速 1//修改docker配置文件# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [&quot;https://s2q9fn53.mirror.aliyuncs.com&quot;], &quot;exec-opts&quot;: [&quot;native.cgroupdriver=systemd&quot;]&#125;# systemctl daemon-reload &amp;&amp; systemctl restart docker 2.3 安装Kubernetes集群1、安装kubelet、kubeadm和kubectl（3台） 1*//查看可用版本，k8s不同版本不兼容# yum list kubelet --showduplicates | sort -r# yum install -y kubelet kubeadm kubectl 2、启动相关服务（3台） 1# systemctl enable kubelet &amp;&amp; systemctl start kubelet &amp;&amp; systemctl status kubelet 2.4 配置Kubernetes集群1、初始化Kubernetes集群（master） 1*//批量打包镜像# docker save $(docker images | grep -v REPOSITORY | awk &#x27;BEGIN&#123;OFS=&quot;:&quot;;ORS=&quot; &quot;&#125;&#123;print $1,$2&#125;&#x27;) -o k8s-master.tar# ll -h k8s-master.tar//导入镜像# docker load -i k8s-master.tar//列出所需镜像版本# kubeadm config images list//初始化（下载镜像等几分钟或导入镜像）# kubeadm init --kubernetes-version=1.22.3 \\--apiserver-advertise-address=192.168.200.10 \\--image-repository registry.aliyuncs.com/google_containers \\--service-cidr=10.10.0.0/16 --pod-network-cidr=10.244.0.0/16##========安装成功返回信息（重要）========##Your Kubernetes control-plane has initialized successfully!To start using your cluster, you need to run the following as a regular user: mkdir -p $HOME/.kube sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config sudo chown $(id -u):$(id -g) $HOME/.kube/configAlternatively, if you are the root user, you can run: export KUBECONFIG=/etc/kubernetes/admin.confYou should now deploy a pod network to the cluster.Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons/Then you can join any number of worker nodes by running the following on each as root:kubeadm join 192.168.200.10:6443 --token ur572b.42fmet5rdbxobldh \\ --discovery-token-ca-cert-hash sha256:de5e2c94a6f7654c298eebc4a0e222330022446743bfe7a3581dce6b5de1d9c9 //查看集群状态*# kubectl get cs========扩展阅读========kubectl get cs显示unhealthy的解决办法https://www.cnblogs.com/wuliping/p/13780147.html=======================//查看pod资源，确保返回的信息都是running;不指定具体的名称空间，表示查看本节点所有的名称空间# kubectl get pod --all-namespaces# kubectl get pods -n kube-system========补充：kubeadm init选项说明======初始化集群需使用kubeadm init命令，可以指定具体参数初始化，也可以指定配置文件初始化。可选参数：①--apiserver-advertise-address：apiserver通告给其他组件的IP地址，一般应该为Master节点的用于集群内部通信的IP地址，0.0.0.0表示节点上所有可用地址②--apiserver-bind-port：apiserver的监听端口，默认是6443③--cert-dir：通讯的ssl证书文件，默认/etc/kubernetes/pki④--control-plane-endpoint：控制台平面的共享终端，可以是负载均衡的ip地址或者dns域名，高可用集群时需要添加⑤--image-repository：拉取镜像的镜像仓库，默认是k8s.gcr.io⑥--kubernetes-version：指定kubernetes版本⑦--pod-network-cidr：pod资源的网段，需与pod网络插件的值设置一致。通常，Flannel网络插件的默认为10.244.0.0/16，Calico插件的默认值为192.168.0.0/16；⑧--service-cidr：service资源的网段⑨--service-dns-domain：service全域名的后缀，默认是cluster.local========问题：怎样找回节点加入集群的token======== 1、忘记token用什么命令找回 # kubeadm token create --print-join-command //一条命令，推荐 2、创建永不过期的token # kubeadm token create --ttl 0 2、使用kubectl命令的环境配置 1//普通用户环境配置$ mkdir -p $HOME/.kube$ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config$ sudo chown $(id -u):$(id -g) $HOME/.kube/config//root user# export KUBECONFIG=/etc/kubernetes/admin.conf//K8S命令补全# source &lt;(kubectl completion bash)========或者========//写法1# echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bash_profile# source .bash_profile//写法2# source /usr/share/bash-completion/bash_completion # source &lt;(kubectl completion bash) # echo &quot;source &lt;(kubectl completion bash)&quot; &gt;&gt; ~/.bashrc========拓展一下=======https://www.cnblogs.com/yuanqiangfei/p/10232148.html 2.5 安装flannel网络（master） 基础知识 https://blog.csdn.net/weixin_45483207/article/details/120189178 github项目 https://github.com/flannel-io/flannel/tree/master/Documentation 1//部署集群后pod网络部署信息 You should now deploy a pod network to the cluster. Run &quot;kubectl apply -f [podnetwork].yaml&quot; with one of the options listed at: https://kubernetes.io/docs/concepts/cluster-administration/addons///验证nodes状态为NoReady# kubectl get nodes//由于DNS污染，需要修改hosts文件，增加下面的解析（网络问题导致安装失败）# cat &gt;&gt; /etc/hosts &lt;&lt;EOF 199.232.68.133 raw.githubusercontent.com199.232.68.133 user-images.githubusercontent.com199.232.68.133 avatars2.githubusercontent.com199.232.68.133 avatars1.githubusercontent.comEOF# kubectl apply -f https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml//验证# kubectl get pods -n kube-system# yum install -y net-tools# ifconfig ========由于网络原因导致flannel网络安装失败========1、手工编写kube-flannel.yml文件2、导入flannel镜像# docker load -i flannel.tar# docker load -i flannel-cni-plugin.tar# kubectl apply -f kube-flannel-new.yml 3、kube-flannel.yml修改quay.io镜像地址解决flannel Init:ImagePullBackOff错误https://www.cnblogs.com/360minitao/p/13237801.html# sed -i &#x27;s/quay.io/quay-mirror.qiniu.com/&#x27; kube-flannel.yml 2.6 节点加入集群（node1、node2）1# kubeadm join 192.168.200.10:6443 --token qhn5x3.3dht86coigln5p2r \\ --discovery-token-ca-cert-hash sha256:29373c4446de02109d37197f256c570ade8ed210ce3d2da0543f64065ecf5be7 //出现错误重新加入# kubeadm reset 2.7 测试运行nginx（master）1、污点 taint：污点的意思。如果一个节点被打上了污点，那么pod是不允许运行在这个节点上面的，默认情况下集群不会在master上调度pod，如果偏想在master上调度Pod，可以执行如下操作： 1## 查看污点# kubectl describe node master|grep -i taintsTaints: &lt;none&gt;##删除默认污点# kubectl taint nodes --all node-role.kubernetes.io/master- 2、部署测试用例 （1）命令方式部署httpd 1# kubectl run httpd-app --image=httpd# kubectl get pod -o wide# curl 10.122.0.7 （2）配置文件部署nginx 1# mkdir /root/kubeadm-ymal# cd /root/kubeadm-ymal/# vi pod.yamlapiVersion: v1kind: Podmetadata: name: nginx labels: app: webspec: containers: - name: nginx image: docker.io/nginx ports: - containerPort: 80# kubectl create -f pod.yaml# kubectl get pod# kubectl get pod -o wide# curl 10.122.2.4 （3）配置文件部署httpd 1# kubectl create deployment nginx1 --image=nginxdeployment.apps/nginx1 created# kubectl expose deployment nginx1 --port=80 --type=NodePortservice/nginx1 exposed# kubectl get pod,svcNAME READY STATUS RESTARTS AGEpod/httpd-app 1/1 Running 0 13mpod/nginx 1/1 Running 0 35mpod/nginx1-b97c459f7-p9z4n 1/1 Running 0 24sNAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGEservice/kubernetes ClusterIP 10.10.0.1 &lt;none&gt; 443/TCP 44mservice/nginx1 NodePort 10.10.218.104 &lt;none&gt; 80:31551/TCP 9s##浏览器访问http://192.168.200.10:31551/ 2.8 安装Dashboard监控（master）1、编辑dashboard的yaml文件 参考文档 https://leheavengame.com/article/61547a4b995acf6bc1e13e85 1//下载yaml，部署dashboard# wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.3.1/aio/deploy/recommended.yaml# mv recommended.yaml dashboard-2.3.1.yaml #本地//创建超级管理员的账号用于登录Dashboard# vim admin-user.yaml #本地//部署dashboard# kubectl apply -f dashboard-2.3.1.yaml -f admin-user.yaml# kubectl get svc -n kubernetes-dashboard# ss -tnl|grep 30002 2、检查相关服务运行状态 1# kubectl get all -n kubernetes-dashboard# kubectl get pod -A | grep kubernetes-dashboard 3、使用浏览器访问 1//任意node节点IP:port（30002）https://192.168.200.10:30002/ 4、查看访问dashboard的认证令牌（token值） 1# kubectl get secret -A | grep admin# kubectl describe secret admin-user-token-28vq6 -n kubernetes-dashboard ##复制token登录 5、创建 kubeconfig 文件以配置对集群的访问权限（扩展） 参考文档 https://leheavengame.com/article/61547a4b995acf6bc1e13e85 2.9 kuboard监控1、安装 1# kubectl apply -f https://kuboard.cn/install-script/kuboard.yaml# kubectl apply -f https://addons.kuboard.cn/metrics-server/0.3.7/metrics-server.yaml 2、查看kuboard运行状态 1# kubectl get pods -l k8s.kuboard.cn/name=kuboard -n kube-systemNAME READY STATUS RESTARTS AGEkuboard-74c645f5df-8lcct 1/1 Running 0 38s 3、获取Token 1# echo $(kubectl -n kube-system get secret $(kubectl -n kube-system get secret | grep kuboard-user | awk &#x27;&#123;print $1&#125;&#x27;) -o go-template=&#x27;&#123;&#123;.data.token&#125;&#125;&#x27; | base64 -d) 4、访问Kuboard 1http://任意一个Worker节点的IP地址:32567/ 新版 https://blog.csdn.net/qq_33709508/article/details/121012914 https://www.kuboard.cn/install/install-k8s.html#%E7%A7%BB%E9%99%A4worker%E8%8A%82%E7%82%B9%E5%B9%B6%E9%87%8D%E8%AF%95 应用： 1、从零开始搭建K8S、Jenkins持续集成环境 https://www.cnblogs.com/frankzhou/p/15407729.html 2、部署LNMP https://blog.csdn.net/qq_41475058/article/details/88898004 https://www.jianshu.com/p/f384660a9615 3、 https://www.cnblogs.com/wuzhenzhao/p/12076827.html 参考安装 https://www.cnblogs.com/yuhangwang/p/11372404.html https://www.cnblogs.com/xiaoyuxixi/p/12142218.html https://www.baidu.com/link?url=s_dp3je1-GA-TtKmqjm8-QNBmjSfuhv37l-AUHypy8JvTg7OITgaLy7fvjBecBoEhVNkV8GJqSyvlMq3C2-d1K&amp;wd=&amp;eqid=a3a8c4a10017051500000003617c0313 https://www.opss.cn/8211.html https://www.jianshu.com/p/f12c9b20b70f https://www.baidu.com/link?url=PUIIUrGwHGBV-IOz719l6uS-TpfPECxOG2QbdxPbo5QcHNX017bMpN_djkPJ0oDugmS-ZuOntHaXsdmz8mvj8a&amp;wd=&amp;eqid=ba462c570014d29500000003617bff9a https://www.cnblogs.com/zhangzhide/p/14760210.html(简单测试)","categories":[],"tags":[]},{"title":"docker私有仓库","slug":"docker私有仓库","date":"2022-01-13T15:41:30.000Z","updated":"2022-02-22T10:15:47.107Z","comments":true,"path":"2022/01/13/docker私有仓库/","link":"","permalink":"http://example.com/2022/01/13/docker%E7%A7%81%E6%9C%89%E4%BB%93%E5%BA%93/","excerpt":"Docker私有仓库1 概述官方的Docker hub（hub.docker.com）用于管理公共镜像，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景（网速、安全）需要我们搭建私有的镜像仓库用于管理我们自己的镜像。","text":"Docker私有仓库1 概述官方的Docker hub（hub.docker.com）用于管理公共镜像，我们可以在上面找到我们想要的镜像，也可以把我们自己的镜像推送上去。但是，有时候，我们的使用场景（网速、安全）需要我们搭建私有的镜像仓库用于管理我们自己的镜像。 Docker hub上提供了registry的镜像，搭建我们自己的私有仓库服务。 Docker Registry由三个部分组成：index，registry，registry client。 Index认为是负责登录、负责认证、负责存储镜像信息和负责对外显示的外部实现 registry则是负责存储镜像的内部实现 Registry Client则是docker客户端。 2 搭建私有仓库-Registry2.1 实验环境准备 主机名 IP 角色 master 192.168.200.10 docker私有仓库服务器 node1 192.168.200.11 客户端（需要安装docker-ce，上传镜像） node2 192.168.200.12 客户端（需要安装Docker-ce，下载镜像） 1、master主机 1234567891011121314151617181920212223242526272829303132333435363738394041##配置网络[root@localhost ~]# vi /etc/sysconfig/network-scripts/ifcfg-ens32TYPE=&quot;Ethernet&quot;BOOTPROTO=&quot;none&quot;NAME=&quot;ens32&quot;DEVICE=&quot;ens32&quot;ONBOOT=&quot;yes&quot;IPADDR=&quot;192.168.200.10&quot;PREFIX=&quot;24&quot;GATEWAY=&quot;192.168.200.2&quot;DNS1=&quot;114.114.114.114&quot;[root@centos ~]# systemctl restart network[root@localhost ~]# ip a[root@localhost ~]# ping www.baidu.com##配置主机名[root@localhost ~]# hostnamectl set-hostname master[root@localhost ~]# bash##关闭防火墙和selinux[root@master ~]# systemctl stop firewalld &amp;&amp; systemctl disable firewalld[root@master ~]# sed -i &#x27;s/SELINUX=enforcing/SELINUX=disabled/g&#x27; /etc/selinux/config[root@master ~]# setenforce 0##开启路由转发[root@master ~]# cat &lt;&lt; EOF &gt;&gt; /etc/sysctl.confnet.ipv4.ip_forward = 1EOF[root@master ~]# sysctl -p##安装Docker-ce，修改加速器[root@master ~]# curl -fsSL https://get.docker.com | bash -s docker --mirror Aliyun[root@master ~]# docker --version[root@master ~]# systemctl enable docker &amp;&amp; systemctl start docker[root@master ~]# tee /etc/docker/daemon.json &lt;&lt;-&#x27;EOF&#x27;&#123;&quot;registry-mirrors&quot;: [&quot;https://aq63ygn3.mirror.aliyuncs.com&quot;]&#125;EOF[root@master ~]# systemctl daemon-reload &amp;&amp; systemctl restart docker 2、node1、node2主机 1操作同master 2.2 搭建私有仓库（docker-registry）1、下载私有仓库注册服务器的镜像 1234[root@master ~]# docker pull registry[root@master ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEregistry 2 708bc6af7e5e 8 weeks ago 25.8MB 2、创建注册服务器容器 1234567[root@master ~]# mkdir -p /usr/local/work/registry[root@master ~]# docker run -d --restart=always --privileged=true --name registry -p 5000:5000 -v /usr/local/work/registry/:/var/lib/registry registry[root@master ~]# docker psCONTAINER ID IMAGE COMMAND CREATED STATUS PORTS NAMES1709b2d51ea5 registry:2 &quot;/entrypoint.sh /etc…&quot; About a minute ago Up About a minute 0.0.0.0:5000-&gt;5000/tcp, :::5000-&gt;5000/tcp registry##–restart=always 此模式容器会跟 docker daemon会随着docker服务的重启而自动恢复 3、访问私有仓库，由于刚启动还没有提交镜像，所以这里显示为空。 12[root@master ~]# curl -X GET http://192.168.200.10:5000/v2/_catalog &#123;&quot;repositories&quot;:[]&#125; 2.3 客户端推送测试（node1）1、修改客户端docker的配置文件/etc/docker/daemon.json，支持http访问 1234567891011[root@node1 ~]# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [ &quot;http://hub-mirror.c.163.com&quot;, &quot;https://eqcxmbvw.mirror.aliyuncs.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot; ], &quot;insecure-registries&quot;: [&quot;192.168.200.10:5000&quot;]&#125;[root@docker-app ~]# systemctl daemon-reload &amp;&amp; systemctl restart docker 2、拉取测试镜像 12[root@node1 ~]# docker pull busybox[root@node1 ~]# docker images 3、给镜像busybox添加一个带有私有仓库ip的tag，这样后面才能成功推送到私有仓库： 12345[root@node1 ~]# docker tag busybox:latest 192.168.200.10:5000/busybox:v1[root@node1 ~]# docker imagesREPOSITORY TAG IMAGE ID CREATED SIZEbusybox latest 83aa35aa1c79 11 days ago 1.22MB192.168.8.20:5000/busybox v1 83aa35aa1c79 11 days ago 1.22MB 4、向私有仓库推送镜像 1[root@node1 ~]# docker push 192.168.200.10:5000/busybox:v1The push refers to repository [192.168.200.10:5000/busybox]5b8c72934dfc: Pushed v1: digest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b size: 527[root@node1 ~]# curl -X GET http://192.168.200.10:5000/v2/_catalog &#123;&quot;repositories&quot;:[&quot;busybox&quot;]&#125;root@node1 ~]# docker tag busybox:latest 192.168.200.10:5000/busybox:v2[root@node1 ~]# docker push 192.168.200.10:5000/busybox:v2[root@node1 ~]# curl -X GET http://192.168.200.10:5000/v2/busybox/tags/list&#123;&quot;name&quot;:&quot;busybox&quot;,&quot;tags&quot;:[&quot;v1&quot;,&quot;v2&quot;]&#125; 8.2.4 客户端拉取测试（node2）1、修改客户端docker的配置文件/etc/docker/daemon.json，支持http访问 1[root@node2 ~]# vim /etc/docker/daemon.json&#123; &quot;registry-mirrors&quot;: [ &quot;http://hub-mirror.c.163.com&quot;, &quot;https://eqcxmbvw.mirror.aliyuncs.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot; ], &quot;insecure-registries&quot;: [&quot;192.168.200.10:5000&quot;]&#125;[root@node2 ~]# systemctl daemon-reload &amp;&amp; systemctl restart docker 2、从私有仓库镜像拉取镜像 1#删除本地镜像[root@node2 ~]# docker rmi `docker images -q`[root@node2 ~]# docker images#从私有仓库拉取镜像/运行镜像[root@node2 ~]# docker pull 192.168.200.10:5000/busybox:v1v1: Pulling from busyboxDigest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486bStatus: Downloaded newer image for 192.168.200.10:5000/busybox:v1192.168.200.10:5000/busybox:v1[root@node2 ~]# docker images 3 私有仓库Harbor3.1 概述1、Harbor 仓库介绍 Docker容器应用的开发和运行离不开可靠的镜像管理，虽然Docker官方也提供了公共的镜像仓库，但是从安全和效率等方面考虑，部署私有环境内的Registry也是非常必要的。Harbor是由VMware公司开源的企业级的Docker Registry管理项目，它包括权限管理(RBAC)、LDAP、日志审核、管理界面、自我注册、镜像复制和中文支持等功能。 3.2 master安装harbor1、安装docker-compose 1##方法1：在线安装[root@master ~]# curl -L https://get.daocloud.io/docker/compose/releases/download/1.25.4/docker-compose-`uname -s`-`uname -m` &gt; /usr/local/bin/docker-compose[root@master ~]# chmod +x /usr/local/bin/docker-compose[root@master ~]# ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose[root@master ~]# docker-compose --versiondocker-compose version 1.25.4, build 8d51620a##方法2：离线安装##下载docker-compose离线包##上传docker-compose到/usr/local/bin/，x 2、安装harbor （1）上传harbor打包文件，解包 1[root@master ~]# tar zxvf harbor-offline-installer-v2.3.1.tgz -C /usr/local/[root@master ~]# ls /usr/local/harbor/common.sh harbor.v2.3.1.tar.gz harbor.yml.tmpl install.sh LICENSE prepare （2）导入docker镜像 1[root@master ~]# cd /usr/local/harbor[root@master harbor]# docker image load -i harbor.v2.3.1.tar.gz [root@master harbor]# docker images #导入很多镜像 （3）修改第harbor.yml配置文件 1[root@master harbor]# cp harbor.yml.tmpl harbor.yml[root@master harbor]# vi harbor.yml# Configuration file of Harbor# The IP address or hostname to access admin UI and registry service.# DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients.hostname: 192.168.200.10 #访问IP# http related confighttp: # port for http, default is 80. If https enabled, this port will redirect to https port port: 80# https related config#https: #注释掉https部分 # https port for harbor, default is 443 #port: 443 # The path of cert and key files for nginx #certificate: /your/certificate/path #private_key: /your/private/key/path# # Uncomment following will enable tls communication between all harbor components# internal_tls:# # set enabled to true means internal tls is enabled# enabled: true# # put your cert and key files on dir# dir: /etc/harbor/tls/internal# Uncomment external_url if you want to enable external proxy# And when it enabled the hostname will no longer used# external_url: https://reg.mydomain.com:8433# The initial password of Harbor admin #登录帐号admin# It only works in first time to install harbor# Remember Change the admin password from UI after launching Harbor.harbor_admin_password: Harbor12345 #登录密码# Harbor DB configurationdatabase: # The password for the root user of Harbor DB. Change this before any production use. password: root123 # The maximum number of connections in the idle connection pool. If it &lt;=0, no idle connections are retained. max_idle_conns: 100 # The maximum number of open connections to the database. If it &lt;= 0, then there is no limit on the number of open connections. # Note: the default number of connections is 1024 for postgres of harbor. max_open_conns: 900# The default data volumedata_volume: /data…… 注意： 1、安装v2.x系列，必须先导入镜像，再拷贝harbor.yml.tmpl模板，顺序不能乱，并且直接使用harbor.yml.tmpl模板作为配置文件，只能使用harbor.yml 作为配置文件。2、v1.x系列安装没有导入docker镜像这一步，直接修改配置文件harbor.cfg 即可，后面操作一样。 （4）检查harbor 1[root@master harbor]# ./prepare （5）安装harbor 1[root@master harbor]# ./install.sh[Step 0]: checking if docker is installed ...Note: docker version: 20.10.8……Creating nginx ... doneCreating harbor-jobservice ... done✔ ----Harbor has been installed and started successfully.---- （6）查看服务器运行的容器 1[root@master harbor]# docker ps （7）访问harbor 1http://192.168.200.10/ ##建议chrome浏览器 （8）harbor管理命令 1[root@master ~]# cd /usr/local/harbor[root@master ~]# docker-compose up -d # 后台启动，如果容器不存在根据镜像自动创建[root@master ~]# docker-compose down -v # 停止容器并删除容器[root@master ~]# docker-compose start # 启动容器，容器不存在就无法启动，不会自动创建镜像[root@master ~]# docker-compose stop # 停止容器 （9）harbor创建项目 3.3 客户端node1使用仓库1、修改/etc/docker/daemon.json文件和 /usr/lib/systemd/system/docker.service文件 1[root@node1 ~]# vim /etc/docker/daemon.json &#123; &quot;registry-mirrors&quot;: [ &quot;http://hub-mirror.c.163.com&quot;, &quot;https://eqcxmbvw.mirror.aliyuncs.com&quot;, &quot;https://docker.mirrors.ustc.edu.cn&quot; ]&#125;##也可以增加 &quot;insecure-registries&quot;: [&quot;192.168.200.10&quot;]，[root@node1 ~]# vi /usr/lib/systemd/system/docker.serviceExecStart=/usr/bin/dockerd -H fd:// --insecure-registry 192.168.200.10 --containerd=/run/containerd/containerd.sock #13行[root@node1 ~]# systemctl daemon-reload &amp;&amp; systemctl restart docker 2、下载测试镜像 1[root@node1 ~]# docker pull busybox[root@node1 ~]# docker images 3、给镜像打标签 1[root@node1 ~]# docker tag busybox:latest 192.168.200.10/busybox:h1[root@node1 ~]# docker images 3、登录harbor 1[root@node1 ~]# docker login -u admin -p Harbor12345 192.168.200.10WARNING! Using --password via the CLI is insecure. Use --password-stdin.WARNING! Your password will be stored unencrypted in /root/.docker/config.json.Configure a credential helper to remove this warning. Seehttps://docs.docker.com/engine/reference/commandline/login/#credentials-storeLogin Succeeded 4、上传镜像到harbor仓库 1[root@node1 ~]# docker push 192.168.200.10/test/busybox:h1The push refers to repository [192.168.200.10/test/busybox]5b8c72934dfc: Pushed h1: digest: sha256:dca71257cd2e72840a21f0323234bb2e33fea6d949fa0f21c5102146f583486b size: 527 5、下载镜像(node2) 1[root@node2 ~]# docker pull 192.168.200.10/test/busybox:h1[root@node2 ~]# docker images","categories":[],"tags":[]},{"title":"Hello World","slug":"hello-world","date":"2022-01-13T15:30:32.005Z","updated":"2022-01-13T15:30:32.005Z","comments":true,"path":"2022/01/13/hello-world/","link":"","permalink":"http://example.com/2022/01/13/hello-world/","excerpt":"","text":"Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new &quot;My New Post&quot; More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment","categories":[],"tags":[]}],"categories":[],"tags":[]}